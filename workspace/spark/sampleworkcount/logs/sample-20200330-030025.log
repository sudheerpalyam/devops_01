20/03/30 03:00:27 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
20/03/30 03:00:27 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
Processing data file :/user/d917355/oozieapps/ooziesparkpi/coordinator.xml
20/03/30 03:00:28 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
20/03/30 03:00:28 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
20/03/30 03:00:28 INFO SparkContext: Running Spark version 2.3.2.3.1.5.0-152
20/03/30 03:00:28 INFO SparkContext: Submitted application: Simple Spark Hive Application
20/03/30 03:00:28 INFO SecurityManager: Changing view acls to: d917355
20/03/30 03:00:28 INFO SecurityManager: Changing modify acls to: d917355
20/03/30 03:00:28 INFO SecurityManager: Changing view acls groups to: 
20/03/30 03:00:28 INFO SecurityManager: Changing modify acls groups to: 
20/03/30 03:00:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls enabled; users  with view permissions: Set(d917355); groups with view permissions: Set(); users  with modify permissions: Set(d917355); groups with modify permissions: Set()
20/03/30 03:00:29 INFO Utils: Successfully started service 'sparkDriver' on port 34023.
20/03/30 03:00:29 INFO SparkEnv: Registering MapOutputTracker
20/03/30 03:00:29 INFO SparkEnv: Registering BlockManagerMaster
20/03/30 03:00:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/03/30 03:00:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/03/30 03:00:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-57cdec4a-5ef6-4522-a716-5a582a74c458
20/03/30 03:00:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
20/03/30 03:00:29 INFO SparkEnv: Registering OutputCommitCoordinator
20/03/30 03:00:29 INFO log: Logging initialized @3486ms
20/03/30 03:00:29 INFO Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
20/03/30 03:00:29 INFO Server: Started @3563ms
20/03/30 03:00:29 INFO SslContextFactory: x509=X509@4303b7f0(localhost,h=[lxdsydstl-lxe04-s01-ehc10001.s01.oan],w=[]) for SslContextFactory@757529a4(file:///etc/security/serverKeys/spark2_keystore.jks,null)
20/03/30 03:00:29 INFO AbstractConnector: Started ServerConnector@23d53b6b{SSL,[ssl, http/1.1]}{0.0.0.0:4523}
20/03/30 03:00:29 INFO Utils: Successfully started service 'SparkUI (HTTPS)' on port 4523.
20/03/30 03:00:29 INFO AbstractConnector: Started ServerConnector@7c9b78e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4123}
20/03/30 03:00:29 INFO Utils: Successfully started service 'SparkUI' on port 4123.
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.h.ContextHandler@1ec7d8b3{/,null,AVAILABLE,@HttpsRedirect}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@585c13de{/jobs,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4463d9d3{/jobs/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@43b0ade{/jobs/job,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1517f633{/jobs/job/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4fe01803{/stages,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@13d186db{/stages/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6f6962ba{/stages/stage,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@15723761{/stages/stage/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@312afbc7{/stages/pool,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@599f571f{/stages/pool/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7b60c3e{/storage,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3fbfa96{/storage/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6569dded{/storage/rdd,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@466d49f0{/storage/rdd/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@710d7aff{/environment,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d7e1102{/environment/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@65327f5{/executors,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2adddc06{/executors/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@301d8120{/executors/threadDump,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6d367020{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@72458efc{/static,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2e3a5237{/,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4ebadd3d{/api,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2d0bfb24{/jobs/job/kill,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@c3fa05a{/stages/stage/kill,null,AVAILABLE,@Spark}
20/03/30 03:00:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://lxdsydstl-lxe04-s01-ehc10001.s01.oan:4123
20/03/30 03:00:29 INFO SparkContext: Added JAR file:/home/d917355/workspace/spark/sampleworkcount/sparkwordcount-0.0.1-SNAPSHOT.jar at spark://lxdsydstl-lxe04-s01-ehc10001.s01.oan:34023/jars/sparkwordcount-0.0.1-SNAPSHOT.jar with timestamp 1585537229938
20/03/30 03:00:31 INFO RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]
20/03/30 03:00:31 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/30 03:00:31 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm1]
20/03/30 03:00:31 INFO Client: Requesting a new application from cluster with 8 NodeManagers
20/03/30 03:00:31 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.5.0-152/0/resource-types.xml
20/03/30 03:00:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (28672 MB per container)
20/03/30 03:00:31 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead
20/03/30 03:00:31 INFO Client: Setting up container launch context for our AM
20/03/30 03:00:31 INFO Client: Setting up the launch environment for our AM container
20/03/30 03:00:31 INFO Client: Preparing resources for our AM container
20/03/30 03:00:31 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_740202471_1, ugi=d917355@S01.OAN (auth:KERBEROS)]]
20/03/30 03:00:31 INFO DFSClient: Created token for d917355: HDFS_DELEGATION_TOKEN owner=d917355@S01.OAN, renewer=yarn, realUser=, issueDate=1585537231456, maxDate=1586142031456, sequenceNumber=2394, masterKeyId=213 on ha-hdfs:ONE-ANALYTICS
20/03/30 03:00:31 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://https@lxdsydstl-lxm01-s01-mhm10001.s01.oan:9393/kms, Ident: (kms-dt owner=d917355, renewer=yarn, realUser=, issueDate=1585537231765, maxDate=1586142031765, sequenceNumber=51, masterKeyId=7))
20/03/30 03:00:32 WARN HiveConf: HiveConf of name hive.llap.daemon.service.hosts does not exist
20/03/30 03:00:32 WARN HiveConf: HiveConf of name hive.server2.http.endpoint does not exist
20/03/30 03:00:33 INFO HiveStreamingCredentialProvider: Obtaining delegation token (secure metastore) for hive streaming..
20/03/30 03:00:33 INFO HiveConf: Found configuration file file:/etc/spark2/3.1.5.0-152/0/hive-site.xml
20/03/30 03:00:33 WARN HiveConf: HiveConf of name hive.server2.http.endpoint does not exist
20/03/30 03:00:33 INFO HiveStreamingCredentialProvider: Getting Hive delegation token for d917355@S01.OAN against hive/_HOST@S01.OAN at thrift://lxdsydstl-lxm01-s01-mhm10001.s01.oan:9083,thrift://lxdsydstl-lxm02-s01-mhm20001.s01.oan:9083
20/03/30 03:00:34 INFO HiveMetaStoreClient: Trying to connect to metastore with URI thrift://lxdsydstl-lxm02-s01-mhm20001.s01.oan:9083
20/03/30 03:00:34 INFO HiveMetaStoreClient: HMSC::open(): Could not find delegation token. Creating KERBEROS-based thrift connection.
20/03/30 03:00:34 INFO HiveMetaStoreClient: Opened a connection to metastore, current connections: 1
20/03/30 03:00:34 INFO HiveMetaStoreClient: Connected to metastore.
20/03/30 03:00:34 INFO HiveStreamingCredentialProvider: Added delegation token (secure metastore) for hive streaming: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 0f 64 39 31 37 33 35 35 40 53 30 31 2e 4f 41 4e 04 68 69 76 65 0f 64 39 31 37 33 35 35 40 53 30 31 2e 4f 41 4e 8a 01 71 29 62 34 d2 8a 01 71 4d 6e b8 d2 8e 0a 08 8e 01 83 alias: HIVE_DELEGATION_TOKEN
20/03/30 03:00:34 INFO HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
20/03/30 03:00:34 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://ONE-ANALYTICS/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/30 03:00:34 INFO Client: Source and destination file systems are the same. Not copying hdfs://ONE-ANALYTICS/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/30 03:00:34 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://ONE-ANALYTICS/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-hive-archive.tar.gz
20/03/30 03:00:34 INFO Client: Source and destination file systems are the same. Not copying hdfs://ONE-ANALYTICS/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-hive-archive.tar.gz
20/03/30 03:00:34 INFO Client: Uploading resource file:/usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.5.0-152.jar -> hdfs://ONE-ANALYTICS/user/d917355/.sparkStaging/application_1585509150056_0007/hive-warehouse-connector-assembly-1.0.0.3.1.5.0-152.jar
20/03/30 03:00:38 INFO Client: Uploading resource file:/home/d917355/workspace/spark/sampleworkcount/log4j.properties -> hdfs://ONE-ANALYTICS/user/d917355/.sparkStaging/application_1585509150056_0007/log4j.properties
20/03/30 03:00:38 INFO Client: Uploading resource file:/tmp/spark-61ef9f45-ec9d-4a96-98e4-1ef8faf320d1/__spark_conf__289383928514876288.zip -> hdfs://ONE-ANALYTICS/user/d917355/.sparkStaging/application_1585509150056_0007/__spark_conf__.zip
20/03/30 03:00:38 INFO SecurityManager: Changing view acls to: d917355
20/03/30 03:00:38 INFO SecurityManager: Changing modify acls to: d917355
20/03/30 03:00:38 INFO SecurityManager: Changing view acls groups to: 
20/03/30 03:00:38 INFO SecurityManager: Changing modify acls groups to: 
20/03/30 03:00:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls enabled; users  with view permissions: Set(d917355); groups with view permissions: Set(); users  with modify permissions: Set(d917355); groups with modify permissions: Set()
20/03/30 03:00:38 INFO Client: Submitting application application_1585509150056_0007 to ResourceManager
20/03/30 03:00:38 INFO YarnClientImpl: Submitted application application_1585509150056_0007
20/03/30 03:00:38 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1585509150056_0007 and attemptId None
20/03/30 03:00:39 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:39 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: adhoc
	 start time: 1585537238357
	 final status: UNDEFINED
	 tracking URL: https://lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090/proxy/application_1585509150056_0007/
	 user: d917355
20/03/30 03:00:40 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:41 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:42 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:43 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:44 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:45 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:46 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:47 INFO Client: Application report for application_1585509150056_0007 (state: ACCEPTED)
20/03/30 03:00:47 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> lxdsydstl-lxm01-s01-mhm10001.s01.oan,lxdsydstl-lxm02-s01-mhm20001.s01.oan, PROXY_URI_BASES -> https://lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090/proxy/application_1585509150056_0007,https://lxdsydstl-lxm02-s01-mhm20001.s01.oan:8090/proxy/application_1585509150056_0007, RM_HA_URLS -> lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090,lxdsydstl-lxm02-s01-mhm20001.s01.oan:8090), /proxy/application_1585509150056_0007
20/03/30 03:00:47 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.
20/03/30 03:00:48 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
20/03/30 03:00:48 INFO Client: Application report for application_1585509150056_0007 (state: RUNNING)
20/03/30 03:00:48 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 172.16.16.18
	 ApplicationMaster RPC port: 0
	 queue: adhoc
	 start time: 1585537238357
	 final status: UNDEFINED
	 tracking URL: https://lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090/proxy/application_1585509150056_0007/
	 user: d917355
20/03/30 03:00:48 INFO YarnClientSchedulerBackend: Application application_1585509150056_0007 has started running.
20/03/30 03:00:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35267.
20/03/30 03:00:48 INFO NettyBlockTransferService: Server created on lxdsydstl-lxe04-s01-ehc10001.s01.oan:35267
20/03/30 03:00:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/03/30 03:00:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, lxdsydstl-lxe04-s01-ehc10001.s01.oan, 35267, None)
20/03/30 03:00:48 INFO BlockManagerMasterEndpoint: Registering block manager lxdsydstl-lxe04-s01-ehc10001.s01.oan:35267 with 366.3 MB RAM, BlockManagerId(driver, lxdsydstl-lxe04-s01-ehc10001.s01.oan, 35267, None)
20/03/30 03:00:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, lxdsydstl-lxe04-s01-ehc10001.s01.oan, 35267, None)
20/03/30 03:00:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, lxdsydstl-lxe04-s01-ehc10001.s01.oan, 35267, None)
20/03/30 03:00:48 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.
20/03/30 03:00:48 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@38089dae{/metrics/json,null,AVAILABLE,@Spark}
20/03/30 03:00:48 INFO EventLoggingListener: Logging events to hdfs:/spark2-history/application_1585509150056_0007
20/03/30 03:00:59 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.16.16.7:39934) with ID 1
20/03/30 03:00:59 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
20/03/30 03:00:59 INFO BlockManagerMasterEndpoint: Registering block manager lxdsydstl-lxw01-s01-whw10002.s01.oan:35823 with 2004.6 MB RAM, BlockManagerId(1, lxdsydstl-lxw01-s01-whw10002.s01.oan, 35823, None)
20/03/30 03:00:59 INFO SharedState: loading hive config file: file:/etc/spark2/3.1.5.0-152/0/hive-site.xml
20/03/30 03:00:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/apps/spark/warehouse').
20/03/30 03:00:59 INFO SharedState: Warehouse path is '/apps/spark/warehouse'.
20/03/30 03:00:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.
20/03/30 03:00:59 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2e3cd732{/SQL,null,AVAILABLE,@Spark}
20/03/30 03:00:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.
20/03/30 03:00:59 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2ab8589a{/SQL/json,null,AVAILABLE,@Spark}
20/03/30 03:00:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.
20/03/30 03:00:59 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@d3cce46{/SQL/execution,null,AVAILABLE,@Spark}
20/03/30 03:00:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.
20/03/30 03:00:59 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@52b30054{/SQL/execution/json,null,AVAILABLE,@Spark}
20/03/30 03:00:59 INFO JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.
20/03/30 03:00:59 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5e51ec2e{/static/sql,null,AVAILABLE,@Spark}
20/03/30 03:00:59 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/03/30 03:00:59 INFO HWConf: Using HS2 URL: jdbc:hive2://lxdsydstl-lxm02-s01-mhm20001.s01.oan:2181,lxdsydstl-lxm03-s01-mhm30001.s01.oan:2181,lxdsydstl-lxm04-s01-mhm40001.s01.oan:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-interactive;principal=hive/_HOST@S01.OAN
20/03/30 03:00:59 INFO HiveWarehouseSessionImpl: Created a new HWC session: 96578c1d-304c-4994-b334-dfabbd2b6b6c
Show Databases: 
20/03/30 03:01:00 INFO CuratorFrameworkImpl: Starting
20/03/30 03:01:00 INFO ZooKeeper: Client environment:zookeeper.version=3.4.6-152--1, built on 12/12/2019 19:33 GMT
20/03/30 03:01:00 INFO ZooKeeper: Client environment:host.name=lxdsydstl-lxe04-s01-ehc10001.s01.oan
20/03/30 03:01:00 INFO ZooKeeper: Client environment:java.version=1.8.0_112
20/03/30 03:01:00 INFO ZooKeeper: Client environment:java.vendor=Oracle Corporation
20/03/30 03:01:00 INFO ZooKeeper: Client environment:java.home=/usr/java/jdk1.8.0_112/jre
20/03/30 03:01:00 INFO ZooKeeper: Client environment:java.class.path=/usr/hdp/current/spark2-client/conf/:/usr/hdp/current/spark2-client/jars/guice-servlet-4.0.jar:/usr/hdp/current/spark2-client/jars/failureaccess-1.0.1.jar:/usr/hdp/current/spark2-client/jars/eigenbase-properties-1.1.5.jar:/usr/hdp/current/spark2-client/jars/spark-repl_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/woodstox-core-5.0.3.jar:/usr/hdp/current/spark2-client/jars/spark-core_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/azure-storage-7.0.0.jar:/usr/hdp/current/spark2-client/jars/jackson-core-2.10.0.jar:/usr/hdp/current/spark2-client/jars/checker-qual-2.8.1.jar:/usr/hdp/current/spark2-client/jars/hadoop-mapreduce-client-core-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/gson-2.2.4.jar:/usr/hdp/current/spark2-client/jars/commons-configuration2-2.1.1.jar:/usr/hdp/current/spark2-client/jars/calcite-core-1.2.0-incubating.jar:/usr/hdp/current/spark2-client/jars/stax-api-1.0.1.jar:/usr/hdp/current/spark2-client/jars/okio-1.6.0.jar:/usr/hdp/current/spark2-client/jars/jackson-databind-2.10.0.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-common-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/lz4-java-1.4.0.jar:/usr/hdp/current/spark2-client/jars/jackson-jaxrs-base-2.10.0.jar:/usr/hdp/current/spark2-client/jars/jersey-common-2.22.2.jar:/usr/hdp/current/spark2-client/jars/jackson-core-asl-1.9.13.jar:/usr/hdp/current/spark2-client/jars/commons-io-2.4.jar:/usr/hdp/current/spark2-client/jars/hk2-locator-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/RoaringBitmap-0.5.11.jar:/usr/hdp/current/spark2-client/jars/metrics-core-3.1.5.jar:/usr/hdp/current/spark2-client/jars/xz-1.0.jar:/usr/hdp/current/spark2-client/jars/jodd-core-3.5.2.jar:/usr/hdp/current/spark2-client/jars/shapeless_2.11-2.3.2.jar:/usr/hdp/current/spark2-client/jars/aircompressor-0.8.jar:/usr/hdp/current/spark2-client/jars/ST4-4.0.4.jar:/usr/hdp/current/spark2-client/jars/gcs-connector-hadoop3-1.9.17.3.1.5.0-152-shaded.jar:/usr/hdp/current/spark2-client/jars/log4j-1.2.17.jar:/usr/hdp/current/spark2-client/jars/hive-exec-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/accessors-smart-1.2.jar:/usr/hdp/current/spark2-client/jars/metrics-jvm-3.1.5.jar:/usr/hdp/current/spark2-client/jars/commons-math3-3.4.1.jar:/usr/hdp/current/spark2-client/jars/dnsjava-2.1.7.jar:/usr/hdp/current/spark2-client/jars/parquet-encoding-1.8.3.jar:/usr/hdp/current/spark2-client/jars/commons-daemon-1.0.13.jar:/usr/hdp/current/spark2-client/jars/jackson-dataformat-cbor-2.10.0.jar:/usr/hdp/current/spark2-client/jars/javax.ws.rs-api-2.0.1.jar:/usr/hdp/current/spark2-client/jars/commons-compiler-3.0.8.jar:/usr/hdp/current/spark2-client/jars/arrow-format-0.8.0.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/hadoop-cloud-storage-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/kerb-identity-1.0.1.jar:/usr/hdp/current/spark2-client/jars/apache-log4j-extras-1.2.17.jar:/usr/hdp/current/spark2-client/jars/avro-mapred-1.7.7-hadoop2.jar:/usr/hdp/current/spark2-client/jars/kerb-core-1.0.1.jar:/usr/hdp/current/spark2-client/jars/metrics-json-3.1.5.jar:/usr/hdp/current/spark2-client/jars/azure-data-lake-store-sdk-2.3.3.jar:/usr/hdp/current/spark2-client/jars/JavaEWAH-0.3.2.jar:/usr/hdp/current/spark2-client/jars/jta-1.1.jar:/usr/hdp/current/spark2-client/jars/j2objc-annotations-1.3.jar:/usr/hdp/current/spark2-client/jars/spark-graphx_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/transaction-api-1.1.jar:/usr/hdp/current/spark2-client/jars/antlr-2.7.7.jar:/usr/hdp/current/spark2-client/jars/hive-beeline-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/paranamer-2.8.jar:/usr/hdp/current/spark2-client/jars/spark-hive-thriftserver_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/wildfly-openssl-1.0.7.Final.jar:/usr/hdp/current/spark2-client/jars/commons-beanutils-1.9.4.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-registry-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/hadoop-azure-datalake-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hdp/current/spark2-client/jars/jackson-jaxrs-json-provider-2.10.0.jar:/usr/hdp/current/spark2-client/jars/protobuf-java-2.5.0.jar:/usr/hdp/current/spark2-client/jars/commons-lang3-3.5.jar:/usr/hdp/current/spark2-client/jars/commons-pool-1.5.4.jar:/usr/hdp/current/spark2-client/jars/spire-macros_2.11-0.13.0.jar:/usr/hdp/current/spark2-client/jars/janino-3.0.8.jar:/usr/hdp/current/spark2-client/jars/scala-reflect-2.11.12.jar:/usr/hdp/current/spark2-client/jars/jsp-api-2.1.jar:/usr/hdp/current/spark2-client/jars/parquet-jackson-1.8.3.jar:/usr/hdp/current/spark2-client/jars/jackson-module-jaxb-annotations-2.10.0.jar:/usr/hdp/current/spark2-client/jars/breeze_2.11-0.13.2.jar:/usr/hdp/current/spark2-client/jars/HikariCP-java7-2.4.12.jar:/usr/hdp/current/spark2-client/jars/kryo-shaded-3.0.3.jar:/usr/hdp/current/spark2-client/jars/slf4j-log4j12-1.7.16.jar:/usr/hdp/current/spark2-client/jars/hive-cli-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/commons-logging-1.1.3.jar:/usr/hdp/current/spark2-client/jars/json4s-jackson_2.11-3.2.11.jar:/usr/hdp/current/spark2-client/jars/spark-kvstore_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/parquet-column-1.8.3.jar:/usr/hdp/current/spark2-client/jars/pyrolite-4.13.jar:/usr/hdp/current/spark2-client/jars/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hdp/current/spark2-client/jars/macro-compat_2.11-1.1.1.jar:/usr/hdp/current/spark2-client/jars/jackson-module-paranamer-2.10.0.jar:/usr/hdp/current/spark2-client/jars/slf4j-api-1.7.16.jar:/usr/hdp/current/spark2-client/jars/aws-java-sdk-bundle-1.11.375.jar:/usr/hdp/current/spark2-client/jars/hadoop-auth-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/zookeeper-3.4.6.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/avro-1.7.7.jar:/usr/hdp/current/spark2-client/jars/ivy-2.4.0.jar:/usr/hdp/current/spark2-client/jars/guava-28.0-jre.jar:/usr/hdp/current/spark2-client/jars/datanucleus-core-4.1.6.jar:/usr/hdp/current/spark2-client/jars/okhttp-2.7.5.jar:/usr/hdp/current/spark2-client/jars/parquet-common-1.8.3.jar:/usr/hdp/current/spark2-client/jars/httpcore-4.4.8.jar:/usr/hdp/current/spark2-client/jars/spark-catalyst_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/parquet-hadoop-1.8.3.jar:/usr/hdp/current/spark2-client/jars/calcite-avatica-1.2.0-incubating.jar:/usr/hdp/current/spark2-client/jars/parquet-hadoop-bundle-1.6.0.jar:/usr/hdp/current/spark2-client/jars/curator-framework-2.12.0.jar:/usr/hdp/current/spark2-client/jars/guice-4.0.jar:/usr/hdp/current/spark2-client/jars/jdo-api-3.0.1.jar:/usr/hdp/current/spark2-client/jars/antlr-runtime-3.4.jar:/usr/hdp/current/spark2-client/jars/hadoop-mapreduce-client-jobclient-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/hadoop-annotations-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/json4s-core_2.11-3.2.11.jar:/usr/hdp/current/spark2-client/jars/datanucleus-rdbms-4.1.7.jar:/usr/hdp/current/spark2-client/jars/zstd-jni-1.3.2-2.jar:/usr/hdp/current/spark2-client/jars/jul-to-slf4j-1.7.16.jar:/usr/hdp/current/spark2-client/jars/scala-xml_2.11-1.0.5.jar:/usr/hdp/current/spark2-client/jars/animal-sniffer-annotations-1.17.jar:/usr/hdp/current/spark2-client/jars/jakarta.activation-api-1.2.1.jar:/usr/hdp/current/spark2-client/jars/flatbuffers-1.2.0-3f79e055.jar:/usr/hdp/current/spark2-client/jars/httpclient-4.5.4.jar:/usr/hdp/current/spark2-client/jars/kerb-admin-1.0.1.jar:/usr/hdp/current/spark2-client/jars/osgi-resource-locator-1.0.1.jar:/usr/hdp/current/spark2-client/jars/hadoop-client-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/hdp/current/spark2-client/jars/stax2-api-3.1.4.jar:/usr/hdp/current/spark2-client/jars/commons-compress-1.4.1.jar:/usr/hdp/current/spark2-client/jars/hppc-0.7.2.jar:/usr/hdp/current/spark2-client/jars/jtransforms-2.4.0.jar:/usr/hdp/current/spark2-client/jars/jackson-module-scala_2.11-2.10.0.jar:/usr/hdp/current/spark2-client/jars/aopalliance-1.0.jar:/usr/hdp/current/spark2-client/jars/javax.inject-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/bcpkix-jdk15on-1.60.jar:/usr/hdp/current/spark2-client/jars/kerby-asn1-1.0.1.jar:/usr/hdp/current/spark2-client/jars/super-csv-2.2.0.jar:/usr/hdp/current/spark2-client/jars/joda-time-2.9.3.jar:/usr/hdp/current/spark2-client/jars/kerby-pkix-1.0.1.jar:/usr/hdp/current/spark2-client/jars/spark-hive_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/commons-codec-1.10.jar:/usr/hdp/current/spark2-client/jars/machinist_2.11-0.6.1.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-api-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/jackson-mapper-asl-1.9.13.jar:/usr/hdp/current/spark2-client/jars/spark-streaming_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-hadoop-cloud_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/orc-mapreduce-1.4.4-nohive.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-client-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/spark2-client/jars/kerb-common-1.0.1.jar:/usr/hdp/current/spark2-client/jars/error_prone_annotations-2.3.2.jar:/usr/hdp/current/spark2-client/jars/kerb-simplekdc-1.0.1.jar:/usr/hdp/current/spark2-client/jars/breeze-macros_2.11-0.13.2.jar:/usr/hdp/current/spark2-client/jars/json4s-ast_2.11-3.2.11.jar:/usr/hdp/current/spark2-client/jars/snappy-java-1.1.2.6.jar:/usr/hdp/current/spark2-client/jars/datanucleus-api-jdo-4.2.1.jar:/usr/hdp/current/spark2-client/jars/kerb-crypto-1.0.1.jar:/usr/hdp/current/spark2-client/jars/commons-httpclient-3.1.jar:/usr/hdp/current/spark2-client/jars/curator-recipes-2.12.0.jar:/usr/hdp/current/spark2-client/jars/arrow-vector-0.8.0.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/kerb-server-1.0.1.jar:/usr/hdp/current/spark2-client/jars/jersey-server-2.22.2.jar:/usr/hdp/current/spark2-client/jars/jersey-guava-2.22.2.jar:/usr/hdp/current/spark2-client/jars/hadoop-azure-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/orc-core-1.4.4-nohive.jar:/usr/hdp/current/spark2-client/jars/aopalliance-repackaged-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/spire_2.11-0.13.0.jar:/usr/hdp/current/spark2-client/jars/jaxb-api-2.2.11.jar:/usr/hdp/current/spark2-client/jars/scala-compiler-2.11.12.jar:/usr/hdp/current/spark2-client/jars/commons-lang-2.6.jar:/usr/hdp/current/spark2-client/jars/py4j-0.10.7.jar:/usr/hdp/current/spark2-client/jars/compress-lzf-1.0.3.jar:/usr/hdp/current/spark2-client/jars/libthrift-0.12.0.jar:/usr/hdp/current/spark2-client/jars/xbean-asm5-shaded-4.4.jar:/usr/hdp/current/spark2-client/jars/javax.servlet-api-3.1.0.jar:/usr/hdp/current/spark2-client/jars/validation-api-1.1.0.Final.jar:/usr/hdp/current/spark2-client/jars/azure-keyvault-core-1.0.0.jar:/usr/hdp/current/spark2-client/jars/kerb-client-1.0.1.jar:/usr/hdp/current/spark2-client/jars/antlr4-runtime-4.7.jar:/usr/hdp/current/spark2-client/jars/spark-mllib-local_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/metrics-graphite-3.1.5.jar:/usr/hdp/current/spark2-client/jars/netty-3.9.9.Final.jar:/usr/hdp/current/spark2-client/jars/hadoop-openstack-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/bcprov-jdk15on-1.60.jar:/usr/hdp/current/spark2-client/jars/derby-10.12.1.1.jar:/usr/hdp/current/spark2-client/jars/spark-yarn_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/hive-jdbc-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/avro-ipc-1.7.7.jar:/usr/hdp/current/spark2-client/jars/stringtemplate-3.2.1.jar:/usr/hdp/current/spark2-client/jars/libfb303-0.9.3.jar:/usr/hdp/current/spark2-client/jars/HikariCP-2.5.1.jar:/usr/hdp/current/spark2-client/jars/stream-2.7.0.jar:/usr/hdp/current/spark2-client/jars/univocity-parsers-2.5.9.jar:/usr/hdp/current/spark2-client/jars/hk2-utils-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/re2j-1.1.jar:/usr/hdp/current/spark2-client/jars/hk2-api-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/jsr305-1.3.9.jar:/usr/hdp/current/spark2-client/jars/calcite-linq4j-1.2.0-incubating.jar:/usr/hdp/current/spark2-client/jars/chill-java-0.8.4.jar:/usr/hdp/current/spark2-client/jars/activation-1.1.1.jar:/usr/hdp/current/spark2-client/jars/hive-metastore-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-mllib_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/snappy-0.2.jar:/usr/hdp/current/spark2-client/jars/spark-network-common_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/arpack_combined_all-0.1.jar:/usr/hdp/current/spark2-client/jars/leveldbjni-all-1.8.jar:/usr/hdp/current/spark2-client/jars/spark-unsafe_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/jcl-over-slf4j-1.7.16.jar:/usr/hdp/current/spark2-client/jars/jersey-media-jaxb-2.22.2.jar:/usr/hdp/current/spark2-client/jars/kerby-util-1.0.1.jar:/usr/hdp/current/spark2-client/jars/commons-dbcp-1.4.jar:/usr/hdp/current/spark2-client/jars/minlog-1.3.0.jar:/usr/hdp/current/spark2-client/jars/netty-all-4.1.42.Final.jar:/usr/hdp/current/spark2-client/jars/jline-2.14.3.jar:/usr/hdp/current/spark2-client/jars/javolution-5.5.1.jar:/usr/hdp/current/spark2-client/jars/jersey-container-servlet-2.22.2.jar:/usr/hdp/current/spark2-client/jars/hadoop-hdfs-client-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/javax.annotation-api-1.2.jar:/usr/hdp/current/spark2-client/jars/scala-library-2.11.12.jar:/usr/hdp/current/spark2-client/jars/arrow-memory-0.8.0.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/kerby-config-1.0.1.jar:/usr/hdp/current/spark2-client/jars/jetty-util-9.3.24.v20180605.jar:/usr/hdp/current/spark2-client/jars/hadoop-common-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/token-provider-1.0.1.jar:/usr/hdp/current/spark2-client/jars/hadoop-mapreduce-client-common-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/mssql-jdbc-6.2.1.jre7.jar:/usr/hdp/current/spark2-client/jars/jcip-annotations-1.0-1.jar:/usr/hdp/current/spark2-client/jars/jersey-client-2.22.2.jar:/usr/hdp/current/spark2-client/jars/commons-collections-3.2.2.jar:/usr/hdp/current/spark2-client/jars/parquet-format-2.3.1.jar:/usr/hdp/current/spark2-client/jars/commons-crypto-1.0.0.jar:/usr/hdp/current/spark2-client/jars/commons-net-2.2.jar:/usr/hdp/current/spark2-client/jars/jersey-container-servlet-core-2.22.2.jar:/usr/hdp/current/spark2-client/jars/jackson-annotations-2.10.0.jar:/usr/hdp/current/spark2-client/jars/objenesis-2.1.jar:/usr/hdp/current/spark2-client/jars/curator-client-2.12.0.jar:/usr/hdp/current/spark2-client/jars/javax.jdo-3.2.0-m3.jar:/usr/hdp/current/spark2-client/jars/javassist-3.18.1-GA.jar:/usr/hdp/current/spark2-client/jars/opencsv-2.3.jar:/usr/hdp/current/spark2-client/jars/scalap-2.11.12.jar:/usr/hdp/current/spark2-client/jars/kerby-xdr-1.0.1.jar:/usr/hdp/current/spark2-client/jars/bonecp-0.8.0.RELEASE.jar:/usr/hdp/current/spark2-client/jars/kerb-util-1.0.1.jar:/usr/hdp/current/spark2-client/jars/spark-sql_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/jetty-xml-9.3.24.v20180605.jar:/usr/hdp/current/spark2-client/jars/core-1.1.2.jar:/usr/hdp/current/spark2-client/jars/hadoop-aws-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-tags_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-launcher_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/commons-cli-1.2.jar:/usr/hdp/current/spark2-client/jars/jpam-1.1.jar:/usr/hdp/current/spark2-client/jars/spark-network-shuffle_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-sketch_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/chill_2.11-0.8.4.jar:/usr/hdp/current/spark2-client/jars/oro-2.0.8.jar:/usr/hdp/current/spark2-client/jars/ehcache-3.3.1.jar:/usr/hdp/current/spark2-client/jars/json-smart-2.3.jar:/usr/hdp/current/spark2-client/jars/jetty-webapp-9.3.24.v20180605.jar:/usr/hdp/current/spark2-client/jars/javax.inject-1.jar:/usr/hdp/current/spark2-client/jars/scala-parser-combinators_2.11-1.1.0.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-server-web-proxy-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/nimbus-jose-jwt-4.41.1.jar:/usr/hdp/current/spark2-client/jars/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-server-common-3.1.1.3.1.5.0-152.jar:/usr/hdp/3.1.5.0-152/hadoop/conf/
20/03/30 03:01:00 INFO ZooKeeper: Client environment:java.library.path=/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64:/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
20/03/30 03:01:00 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp
20/03/30 03:01:00 INFO ZooKeeper: Client environment:java.compiler=<NA>
20/03/30 03:01:00 INFO ZooKeeper: Client environment:os.name=Linux
20/03/30 03:01:00 INFO ZooKeeper: Client environment:os.arch=amd64
20/03/30 03:01:00 INFO ZooKeeper: Client environment:os.version=5.0.0-27-lowlatency
20/03/30 03:01:00 INFO ZooKeeper: Client environment:user.name=d917355
20/03/30 03:01:00 INFO ZooKeeper: Client environment:user.home=/home/d917355
20/03/30 03:01:00 INFO ZooKeeper: Client environment:user.dir=/home/d917355/workspace/spark/sampleworkcount
20/03/30 03:01:00 INFO ZooKeeper: Initiating client connection, connectString=lxdsydstl-lxm02-s01-mhm20001.s01.oan:2181,lxdsydstl-lxm03-s01-mhm30001.s01.oan:2181,lxdsydstl-lxm04-s01-mhm40001.s01.oan:2181 sessionTimeout=60000 watcher=shadecurator.org.apache.curator.ConnectionState@2cdb5974
20/03/30 03:01:00 INFO ClientCnxn: Opening socket connection to server lxdsydstl-lxm02-s01-mhm20001.s01.oan/172.16.16.3:2181. Will not attempt to authenticate using SASL (unknown error)
20/03/30 03:01:00 INFO ClientCnxn: Socket connection established, initiating session, client: /172.16.6.13:54928, server: lxdsydstl-lxm02-s01-mhm20001.s01.oan/172.16.16.3:2181
20/03/30 03:01:00 INFO ClientCnxn: Session establishment complete on server lxdsydstl-lxm02-s01-mhm20001.s01.oan/172.16.16.3:2181, sessionid = 0x17090a10256a849, negotiated timeout = 60000
20/03/30 03:01:00 INFO ConnectionStateManager: State change: CONNECTED
20/03/30 03:01:00 INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting
20/03/30 03:01:00 INFO ZooKeeper: Session: 0x17090a10256a849 closed
20/03/30 03:01:00 INFO ClientCnxn: EventThread shut down
20/03/30 03:01:00 INFO HiveConnection: Connected to lxdsydstl-lxe05-s01-ehc20001.s01.oan:10501
20/03/30 03:01:00 INFO CuratorFrameworkImpl: Starting
20/03/30 03:01:00 INFO ZooKeeper: Initiating client connection, connectString=lxdsydstl-lxm02-s01-mhm20001.s01.oan:2181,lxdsydstl-lxm03-s01-mhm30001.s01.oan:2181,lxdsydstl-lxm04-s01-mhm40001.s01.oan:2181 sessionTimeout=60000 watcher=shadecurator.org.apache.curator.ConnectionState@4d0abb23
20/03/30 03:01:00 INFO ClientCnxn: Opening socket connection to server lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181. Will not attempt to authenticate using SASL (unknown error)
20/03/30 03:01:00 INFO ClientCnxn: Socket connection established, initiating session, client: /172.16.6.13:57074, server: lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181
20/03/30 03:01:00 INFO ClientCnxn: Session establishment complete on server lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181, sessionid = 0x37090a10009a71d, negotiated timeout = 60000
20/03/30 03:01:00 INFO ConnectionStateManager: State change: CONNECTED
20/03/30 03:01:00 INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting
20/03/30 03:01:00 INFO ClientCnxn: EventThread shut down
20/03/30 03:01:00 INFO ZooKeeper: Session: 0x37090a10009a71d closed
20/03/30 03:01:00 INFO HiveConnection: Connected to lxdsydstl-lxe05-s01-ehc20001.s01.oan:10501
20/03/30 03:01:02 INFO CodeGenerator: Code generated in 314.41503 ms
20/03/30 03:01:02 INFO CodeGenerator: Code generated in 23.015687 ms
+------------------+
|     database_name|
+------------------+
|         analytics|
|       c_nonsp_nem|
|   c_nsbu_headspin|
|        c_nsbu_mem|
|           default|
|information_schema|
|       o_nonsp_rce|
|        o_nsbu_mem|
|        o_nsbu_rce|
|               sys|
|              test|
|             tpcds|
+------------------+

Show Tables: 
+---------+
| tab_name|
+---------+
|web_sales|
+---------+

Hive Execute select tpcds.web_sales: 
20/03/30 03:01:03 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/03/30 03:01:03 INFO CodeGenerator: Code generated in 116.893948 ms
20/03/30 03:01:03 INFO CodeGenerator: Code generated in 76.234431 ms
+---------------+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+
|ws_sold_date_sk|ws_sold_time_sk|ws_ship_date_sk|ws_item_sk|ws_bill_customer_sk|ws_bill_cdemo_sk|ws_bill_hdemo_sk|ws_bill_addr_sk|ws_ship_customer_sk|ws_ship_cdemo_sk|ws_ship_hdemo_sk|ws_ship_addr_sk|ws_web_page_sk|ws_web_site_sk|ws_ship_mode_sk|ws_warehouse_sk|ws_promo_sk|ws_order_number|ws_quantity|ws_wholesale_cost|ws_list_price|ws_sales_price|ws_ext_discount_amt|ws_ext_sales_price|ws_ext_wholesale_cost|ws_ext_list_price|ws_ext_tax|ws_coupon_amt|ws_ext_ship_cost|ws_net_paid|ws_net_paid_inc_tax|ws_net_paid_inc_ship|ws_net_paid_inc_ship_tax|ws_net_profit|
+---------------+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+
|2451850        |50687          |2451953        |1         |42144              |949560          |176             |25545          |69179              |824534          |146             |47962          |47            |20            |8              |2              |99         |3266           |1          |72.33            |117.89       |68.37         |49.52              |68.37             |72.33                |117.89           |2.05      |0.00         |10.61           |68.37      |70.42              |78.98               |81.03                   |-3.96        |
|2452587        |14861          |2452628        |1         |87047              |1555333         |3619            |12258          |6384               |916331          |6716            |23755          |31            |7             |19             |2              |192        |4207           |61         |94.86            |110.98       |79.90         |1895.88            |4873.90           |5786.46              |6769.78          |292.43    |0.00         |473.36          |4873.90    |5166.33            |5347.26             |5639.69                 |-912.56      |
|2452030        |79811          |2452106        |1         |23635              |825904          |6573            |31595          |1635               |148471          |6402            |37071          |41            |5             |2              |4              |83         |4219           |92         |9.67             |15.66        |8.45          |663.32             |777.40            |889.64               |1440.72          |7.77      |0.00         |201.48          |777.40     |785.17             |978.88              |986.65                  |-112.24      |
|2451886        |55510          |2451958        |1         |45014              |371461          |1108            |30755          |39471              |252529          |2896            |11743          |38            |11            |19             |5              |158        |4347           |30         |96.80            |257.48       |242.03        |463.50             |7260.90           |2904.00              |7724.40          |435.65    |0.00         |2085.30         |7260.90    |7696.55            |9346.20             |9781.85                 |4356.90      |
|2452078        |67183          |2452141        |1         |1322               |1649675         |2028            |11981          |6417               |506560          |2993            |42546          |59            |3             |14             |3              |165        |4583           |52         |51.37            |64.21        |3.21          |3172.00            |166.92            |2671.24              |3338.92          |0.00      |0.00         |1502.28         |166.92     |166.92             |1669.20             |1669.20                 |-2504.32     |
+---------------+---------------+---------------+----------+-------------------+----------------+----------------+---------------+-------------------+----------------+----------------+---------------+--------------+--------------+---------------+---------------+-----------+---------------+-----------+-----------------+-------------+--------------+-------------------+------------------+---------------------+-----------------+----------+-------------+----------------+-----------+-------------------+--------------------+------------------------+-------------+

Hive ExecuteQuery select tpcds.web_sales: 
20/03/30 03:01:03 INFO HiveWarehouseConnector: Found reader configuration - spark.datasource.hive.warehouse.use.spark23x.specific.reader=false, spark.datasource.hive.warehouse.disable.pruning.and.pushdowns=true
20/03/30 03:01:03 INFO HiveWarehouseConnector: Using reader HiveWarehouseDataSourceReader with column pruning and filter pushdown disabled
20/03/30 03:01:03 INFO JobUtil: Assigned handle ID:d8d588cf-a705-4cf7-aa5a-3211dfe6b289 for the current operation.
20/03/30 03:01:03 INFO LlapBaseInputFormat: Handle ID d8d588cf-a705-4cf7-aa5a-3211dfe6b289: query=SELECT * FROM tpcds.web_sales limit 5
20/03/30 03:01:03 INFO CuratorFrameworkImpl: Starting
20/03/30 03:01:03 INFO ZooKeeper: Initiating client connection, connectString=lxdsydstl-lxm02-s01-mhm20001.s01.oan:2181,lxdsydstl-lxm03-s01-mhm30001.s01.oan:2181,lxdsydstl-lxm04-s01-mhm40001.s01.oan:2181 sessionTimeout=60000 watcher=shadecurator.org.apache.curator.ConnectionState@fdf029a
20/03/30 03:01:03 INFO ClientCnxn: Opening socket connection to server lxdsydstl-lxm03-s01-mhm30001.s01.oan/172.16.16.4:2181. Will not attempt to authenticate using SASL (unknown error)
20/03/30 03:01:03 INFO ClientCnxn: Socket connection established, initiating session, client: /172.16.6.13:33748, server: lxdsydstl-lxm03-s01-mhm30001.s01.oan/172.16.16.4:2181
20/03/30 03:01:03 INFO ClientCnxn: Session establishment complete on server lxdsydstl-lxm03-s01-mhm30001.s01.oan/172.16.16.4:2181, sessionid = 0x27090a1002aa912, negotiated timeout = 60000
20/03/30 03:01:03 INFO ConnectionStateManager: State change: CONNECTED
20/03/30 03:01:03 INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting
20/03/30 03:01:03 INFO ZooKeeper: Session: 0x27090a1002aa912 closed
20/03/30 03:01:03 INFO ClientCnxn: EventThread shut down
20/03/30 03:01:03 INFO HiveConnection: Connected to lxdsydstl-lxe05-s01-ehc20001.s01.oan:10501
20/03/30 03:01:04 INFO CodeGenerator: Code generated in 111.843978 ms
20/03/30 03:01:04 INFO HiveWarehouseDataSourceReader: Final Query: select `ws_sold_date_sk` , `ws_sold_time_sk` , `ws_ship_date_sk` , `ws_item_sk` , `ws_bill_customer_sk` , `ws_bill_cdemo_sk` , `ws_bill_hdemo_sk` , `ws_bill_addr_sk` , `ws_ship_customer_sk` , `ws_ship_cdemo_sk` , `ws_ship_hdemo_sk` , `ws_ship_addr_sk` , `ws_web_page_sk` , `ws_web_site_sk` , `ws_ship_mode_sk` , `ws_warehouse_sk` , `ws_promo_sk` , `ws_order_number` , `ws_quantity` , `ws_wholesale_cost` , `ws_list_price` , `ws_sales_price` , `ws_ext_discount_amt` , `ws_ext_sales_price` , `ws_ext_wholesale_cost` , `ws_ext_list_price` , `ws_ext_tax` , `ws_coupon_amt` , `ws_ext_ship_cost` , `ws_net_paid` , `ws_net_paid_inc_tax` , `ws_net_paid_inc_ship` , `ws_net_paid_inc_ship_tax` , `ws_net_profit` from (SELECT * FROM tpcds.web_sales limit 5) as q_077fb3bda21549efacedd91aa2366900 
20/03/30 03:01:04 INFO JobUtil: Assigned handle ID:e5c3b9ae-75a1-4fa9-95d1-e2afa608c1bb for the current operation.
20/03/30 03:01:04 INFO HiveWarehouseDataSourceReader: Additional props for generating splits: null
20/03/30 03:01:04 INFO LlapBaseInputFormat: Handle ID e5c3b9ae-75a1-4fa9-95d1-e2afa608c1bb: query=select `ws_sold_date_sk` , `ws_sold_time_sk` , `ws_ship_date_sk` , `ws_item_sk` , `ws_bill_customer_sk` , `ws_bill_cdemo_sk` , `ws_bill_hdemo_sk` , `ws_bill_addr_sk` , `ws_ship_customer_sk` , `ws_ship_cdemo_sk` , `ws_ship_hdemo_sk` , `ws_ship_addr_sk` , `ws_web_page_sk` , `ws_web_site_sk` , `ws_ship_mode_sk` , `ws_warehouse_sk` , `ws_promo_sk` , `ws_order_number` , `ws_quantity` , `ws_wholesale_cost` , `ws_list_price` , `ws_sales_price` , `ws_ext_discount_amt` , `ws_ext_sales_price` , `ws_ext_wholesale_cost` , `ws_ext_list_price` , `ws_ext_tax` , `ws_coupon_amt` , `ws_ext_ship_cost` , `ws_net_paid` , `ws_net_paid_inc_tax` , `ws_net_paid_inc_ship` , `ws_net_paid_inc_ship_tax` , `ws_net_profit` from (SELECT * FROM tpcds.web_sales limit 5) as q_077fb3bda21549efacedd91aa2366900 
20/03/30 03:01:04 INFO CuratorFrameworkImpl: Starting
20/03/30 03:01:04 INFO ZooKeeper: Initiating client connection, connectString=lxdsydstl-lxm02-s01-mhm20001.s01.oan:2181,lxdsydstl-lxm03-s01-mhm30001.s01.oan:2181,lxdsydstl-lxm04-s01-mhm40001.s01.oan:2181 sessionTimeout=60000 watcher=shadecurator.org.apache.curator.ConnectionState@75559cd9
20/03/30 03:01:04 INFO ClientCnxn: Opening socket connection to server lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181. Will not attempt to authenticate using SASL (unknown error)
20/03/30 03:01:04 INFO ClientCnxn: Socket connection established, initiating session, client: /172.16.6.13:57156, server: lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181
20/03/30 03:01:04 INFO ClientCnxn: Session establishment complete on server lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181, sessionid = 0x37090a10009a71e, negotiated timeout = 60000
20/03/30 03:01:04 INFO ConnectionStateManager: State change: CONNECTED
20/03/30 03:01:04 INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting
20/03/30 03:01:04 INFO ZooKeeper: Session: 0x37090a10009a71e closed
20/03/30 03:01:04 INFO ClientCnxn: EventThread shut down
20/03/30 03:01:04 INFO HiveConnection: Connected to lxdsydstl-lxe05-s01-ehc20001.s01.oan:10501
20/03/30 03:01:06 INFO HiveWarehouseDataSourceReader: Number of splits generated: 3
20/03/30 03:01:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 369.2 KB, free 365.9 MB)
20/03/30 03:01:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 127.3 KB, free 365.8 MB)
20/03/30 03:01:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on lxdsydstl-lxe04-s01-ehc10001.s01.oan:35267 (size: 127.3 KB, free: 366.2 MB)
20/03/30 03:01:06 INFO SparkContext: Created broadcast 0 from broadcast at HiveWarehouseDataSourceReader.java:237
20/03/30 03:01:06 INFO ContextCleaner: Cleaned accumulator 2
20/03/30 03:01:06 INFO ContextCleaner: Cleaned accumulator 0
20/03/30 03:01:06 INFO ContextCleaner: Cleaned accumulator 1
20/03/30 03:01:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.9 KB, free 365.8 MB)
20/03/30 03:01:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 619.0 B, free 365.8 MB)
20/03/30 03:01:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on lxdsydstl-lxe04-s01-ehc10001.s01.oan:35267 (size: 619.0 B, free: 366.2 MB)
20/03/30 03:01:06 INFO SparkContext: Created broadcast 1 from broadcast at HiveWarehouseDataSourceReader.java:237
20/03/30 03:01:06 INFO HiveWarehouseDataSourceReader: Serializing 1 actual splits to send to executors
20/03/30 03:01:06 INFO HiveWarehouseDataSourceReader: Serialized 1 actual splits in 2 millis
20/03/30 03:01:06 INFO HiveWarehouseSessionImpl: Adding resource: com.hortonworks.spark.sql.hive.llap.common.HwcResource@65dd2191 to current session: 96578c1d-304c-4994-b334-dfabbd2b6b6c
20/03/30 03:01:06 INFO SparkContext: Starting job: show at SimpleHiveApp.scala:23
20/03/30 03:01:06 INFO DAGScheduler: Got job 0 (show at SimpleHiveApp.scala:23) with 1 output partitions
20/03/30 03:01:06 INFO DAGScheduler: Final stage: ResultStage 0 (show at SimpleHiveApp.scala:23)
20/03/30 03:01:06 INFO DAGScheduler: Parents of final stage: List()
20/03/30 03:01:06 INFO DAGScheduler: Missing parents: List()
20/03/30 03:01:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at SimpleHiveApp.scala:23), which has no missing parents
20/03/30 03:01:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 33.5 KB, free 365.8 MB)
20/03/30 03:01:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.8 KB, free 365.8 MB)
20/03/30 03:01:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on lxdsydstl-lxe04-s01-ehc10001.s01.oan:35267 (size: 6.8 KB, free: 366.2 MB)
20/03/30 03:01:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1039
20/03/30 03:01:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at SimpleHiveApp.scala:23) (first 15 tasks are for partitions Vector(0))
20/03/30 03:01:06 INFO YarnScheduler: Adding task set 0.0 with 1 tasks
20/03/30 03:01:07 WARN TaskSetManager: Stage 0 contains a task of very large size (123 KB). The maximum recommended task size is 100 KB.
20/03/30 03:01:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1, partition 0, NODE_LOCAL, 126615 bytes)
20/03/30 03:01:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on lxdsydstl-lxw01-s01-whw10002.s01.oan:35823 (size: 6.8 KB, free: 2004.6 MB)
20/03/30 03:01:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on lxdsydstl-lxw01-s01-whw10002.s01.oan:35823 (size: 127.3 KB, free: 2004.5 MB)
20/03/30 03:01:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on lxdsydstl-lxw01-s01-whw10002.s01.oan:35823 (size: 619.0 B, free: 2004.5 MB)
20/03/30 03:01:08 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1): java.lang.RuntimeException: java.io.IOException: No service instances found in registry
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:67)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: No service instances found in registry
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getServiceInstance(LlapBaseInputFormat.java:436)
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getRecordReader(LlapBaseInputFormat.java:160)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.getRecordReader(HiveWarehouseDataReader.java:77)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.<init>(HiveWarehouseDataReader.java:55)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.getDataReader(HiveWarehouseDataReaderFactory.java:73)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:65)
	... 18 more

20/03/30 03:01:08 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1, lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1, partition 0, NODE_LOCAL, 126615 bytes)
20/03/30 03:01:08 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1: java.lang.RuntimeException (java.io.IOException: No service instances found in registry) [duplicate 1]
20/03/30 03:01:08 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2, lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1, partition 0, NODE_LOCAL, 126615 bytes)
20/03/30 03:01:08 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1: java.lang.RuntimeException (java.io.IOException: No service instances found in registry) [duplicate 2]
20/03/30 03:01:08 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3, lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1, partition 0, NODE_LOCAL, 126615 bytes)
20/03/30 03:01:08 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1: java.lang.RuntimeException (java.io.IOException: No service instances found in registry) [duplicate 3]
20/03/30 03:01:08 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
20/03/30 03:01:09 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/03/30 03:01:09 INFO YarnScheduler: Cancelling stage 0
20/03/30 03:01:09 INFO DAGScheduler: ResultStage 0 (show at SimpleHiveApp.scala:23) failed in 2.100 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1): java.lang.RuntimeException: java.io.IOException: No service instances found in registry
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:67)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: No service instances found in registry
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getServiceInstance(LlapBaseInputFormat.java:436)
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getRecordReader(LlapBaseInputFormat.java:160)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.getRecordReader(HiveWarehouseDataReader.java:77)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.<init>(HiveWarehouseDataReader.java:55)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.getDataReader(HiveWarehouseDataReaderFactory.java:73)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:65)
	... 18 more

Driver stacktrace:
20/03/30 03:01:09 INFO DAGScheduler: Job 0 failed: show at SimpleHiveApp.scala:23, took 2.153540 s
20/03/30 03:01:09 INFO HiveWarehouseSessionImpl: Remove and close resource: com.hortonworks.spark.sql.hive.llap.common.HwcResource@65dd2191 from current session: 96578c1d-304c-4994-b334-dfabbd2b6b6c
20/03/30 03:01:09 INFO HwcResource: Closing reader llap resource: e5c3b9ae-75a1-4fa9-95d1-e2afa608c1bb
20/03/30 03:01:09 INFO HwcResource: Closing reader broadcast variables: CommonBroadcastInfo{schemaSplit=Broadcast(1), planSplit=Broadcast(0)}
20/03/30 03:01:09 INFO TorrentBroadcast: Destroying Broadcast(0) (from destroy at HwcResource.java:51)
20/03/30 03:01:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on lxdsydstl-lxe04-s01-ehc10001.s01.oan:35267 in memory (size: 127.3 KB, free: 366.3 MB)
20/03/30 03:01:09 INFO BlockManagerInfo: Removed broadcast_0_piece0 on lxdsydstl-lxw01-s01-whw10002.s01.oan:35823 in memory (size: 127.3 KB, free: 2004.6 MB)
20/03/30 03:01:09 INFO TorrentBroadcast: Destroying Broadcast(1) (from destroy at HwcResource.java:54)
20/03/30 03:01:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on lxdsydstl-lxe04-s01-ehc10001.s01.oan:35267 in memory (size: 619.0 B, free: 366.3 MB)
20/03/30 03:01:09 INFO BlockManagerInfo: Removed broadcast_1_piece0 on lxdsydstl-lxw01-s01-whw10002.s01.oan:35823 in memory (size: 619.0 B, free: 2004.6 MB)
20/03/30 03:01:09 INFO LlapQueryExecutionListener: Closing Hive connection via com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataSourceReader
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, lxdsydstl-lxw01-s01-whw10002.s01.oan, executor 1): java.lang.RuntimeException: java.io.IOException: No service instances found in registry
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:67)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: No service instances found in registry
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getServiceInstance(LlapBaseInputFormat.java:436)
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getRecordReader(LlapBaseInputFormat.java:160)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.getRecordReader(HiveWarehouseDataReader.java:77)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.<init>(HiveWarehouseDataReader.java:55)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.getDataReader(HiveWarehouseDataReaderFactory.java:73)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:65)
	... 18 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2079)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:725)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:702)
	at com.cloudera.sparkwordcount.SimpleHiveApp$.main(SimpleHiveApp.scala:23)
	at com.cloudera.sparkwordcount.SimpleHiveApp.main(SimpleHiveApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:900)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.RuntimeException: java.io.IOException: No service instances found in registry
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:67)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: No service instances found in registry
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getServiceInstance(LlapBaseInputFormat.java:436)
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getRecordReader(LlapBaseInputFormat.java:160)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.getRecordReader(HiveWarehouseDataReader.java:77)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.<init>(HiveWarehouseDataReader.java:55)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.getDataReader(HiveWarehouseDataReaderFactory.java:73)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:65)
	... 18 more
20/03/30 03:01:09 INFO SparkContext: Invoking stop() from shutdown hook
20/03/30 03:01:09 INFO HwcSparkListener: Spark onApplicationEnd event triggered, closing all llap resources
20/03/30 03:01:09 INFO AbstractConnector: Stopped Spark@23d53b6b{SSL,[ssl, http/1.1]}{0.0.0.0:4523}
20/03/30 03:01:09 INFO AbstractConnector: Stopped HttpsRedirect@7c9b78e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4123}
20/03/30 03:01:09 INFO SparkUI: Stopped Spark web UI at http://lxdsydstl-lxe04-s01-ehc10001.s01.oan:4123
20/03/30 03:01:09 INFO YarnClientSchedulerBackend: Interrupting monitor thread
20/03/30 03:01:09 INFO YarnClientSchedulerBackend: Shutting down all executors
20/03/30 03:01:09 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
20/03/30 03:01:09 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices
(serviceOption=None,
 services=List(),
 started=false)
20/03/30 03:01:09 INFO YarnClientSchedulerBackend: Stopped
20/03/30 03:01:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/03/30 03:01:09 INFO MemoryStore: MemoryStore cleared
20/03/30 03:01:09 INFO BlockManager: BlockManager stopped
20/03/30 03:01:09 INFO BlockManagerMaster: BlockManagerMaster stopped
20/03/30 03:01:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/03/30 03:01:09 INFO SparkContext: Successfully stopped SparkContext
20/03/30 03:01:09 INFO ShutdownHookManager: Shutdown hook called
20/03/30 03:01:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-e88f6d6f-a8eb-4a07-a1e7-54505ebb32b4
20/03/30 03:01:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-61ef9f45-ec9d-4a96-98e4-1ef8faf320d1
