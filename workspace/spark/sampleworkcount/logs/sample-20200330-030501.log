20/03/30 03:05:03 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.
20/03/30 03:05:03 WARN SparkConf: The configuration key 'spark.yarn.driver.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.driver.memoryOverhead' instead.
20/03/30 03:05:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/03/30 03:05:05 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
20/03/30 03:05:05 INFO RequestHedgingRMFailoverProxyProvider: Created wrapped proxy for [rm1, rm2]
20/03/30 03:05:05 INFO RequestHedgingRMFailoverProxyProvider: Looking for the active RM in [rm1, rm2]...
20/03/30 03:05:05 INFO RequestHedgingRMFailoverProxyProvider: Found active RM [rm1]
20/03/30 03:05:05 INFO Client: Requesting a new application from cluster with 8 NodeManagers
20/03/30 03:05:05 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.5.0-152/0/resource-types.xml
20/03/30 03:05:05 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (28672 MB per container)
20/03/30 03:05:05 INFO Client: Will allocate AM container, with 5120 MB memory including 4096 MB overhead
20/03/30 03:05:05 INFO Client: Setting up container launch context for our AM
20/03/30 03:05:05 INFO Client: Setting up the launch environment for our AM container
20/03/30 03:05:05 INFO Client: Preparing resources for our AM container
20/03/30 03:05:05 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_540096182_1, ugi=d917355@S01.OAN (auth:KERBEROS)]]
20/03/30 03:05:05 INFO DFSClient: Created token for d917355: HDFS_DELEGATION_TOKEN owner=d917355@S01.OAN, renewer=yarn, realUser=, issueDate=1585537505739, maxDate=1586142305739, sequenceNumber=2398, masterKeyId=213 on ha-hdfs:ONE-ANALYTICS
20/03/30 03:05:06 INFO KMSClientProvider: New token created: (Kind: kms-dt, Service: kms://https@lxdsydstl-lxm03-s01-mhm30001.s01.oan:9393/kms, Ident: (kms-dt owner=d917355, renewer=yarn, realUser=, issueDate=1585537506177, maxDate=1586142306177, sequenceNumber=55, masterKeyId=7))
20/03/30 03:05:06 WARN HiveConf: HiveConf of name hive.llap.daemon.service.hosts does not exist
20/03/30 03:05:06 WARN HiveConf: HiveConf of name hive.server2.http.endpoint does not exist
20/03/30 03:05:06 WARN HiveConf: HiveConf of name hive.llap.daemon.service.hosts does not exist
20/03/30 03:05:06 WARN HiveConf: HiveConf of name hive.server2.http.endpoint does not exist
20/03/30 03:05:06 WARN HiveConf: HiveConf of name hive.server2.http.endpoint does not exist
20/03/30 03:05:06 INFO metastore: Trying to connect to metastore with URI thrift://lxdsydstl-lxm02-s01-mhm20001.s01.oan:9083
20/03/30 03:05:06 INFO metastore: Connected to metastore.
20/03/30 03:05:08 INFO HiveServer2CredentialProvider: Getting HS2 delegation token for d917355 via jdbc:hive2://lxdsydstl-lxm02-s01-mhm20001.s01.oan:2181,lxdsydstl-lxm03-s01-mhm30001.s01.oan:2181,lxdsydstl-lxm04-s01-mhm40001.s01.oan:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-interactive;principal=hive/_HOST@S01.OAN
20/03/30 03:05:08 INFO CuratorFrameworkImpl: Starting
20/03/30 03:05:08 INFO ZooKeeper: Client environment:zookeeper.version=3.4.6-152--1, built on 12/12/2019 19:33 GMT
20/03/30 03:05:08 INFO ZooKeeper: Client environment:host.name=lxdsydstl-lxe04-s01-ehc10001.s01.oan
20/03/30 03:05:08 INFO ZooKeeper: Client environment:java.version=1.8.0_112
20/03/30 03:05:08 INFO ZooKeeper: Client environment:java.vendor=Oracle Corporation
20/03/30 03:05:08 INFO ZooKeeper: Client environment:java.home=/usr/java/jdk1.8.0_112/jre
20/03/30 03:05:08 INFO ZooKeeper: Client environment:java.class.path=/usr/hdp/current/spark2-client/conf/:/usr/hdp/current/spark2-client/jars/guice-servlet-4.0.jar:/usr/hdp/current/spark2-client/jars/failureaccess-1.0.1.jar:/usr/hdp/current/spark2-client/jars/eigenbase-properties-1.1.5.jar:/usr/hdp/current/spark2-client/jars/spark-repl_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/woodstox-core-5.0.3.jar:/usr/hdp/current/spark2-client/jars/spark-core_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/azure-storage-7.0.0.jar:/usr/hdp/current/spark2-client/jars/jackson-core-2.10.0.jar:/usr/hdp/current/spark2-client/jars/checker-qual-2.8.1.jar:/usr/hdp/current/spark2-client/jars/hadoop-mapreduce-client-core-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/gson-2.2.4.jar:/usr/hdp/current/spark2-client/jars/commons-configuration2-2.1.1.jar:/usr/hdp/current/spark2-client/jars/calcite-core-1.2.0-incubating.jar:/usr/hdp/current/spark2-client/jars/stax-api-1.0.1.jar:/usr/hdp/current/spark2-client/jars/okio-1.6.0.jar:/usr/hdp/current/spark2-client/jars/jackson-databind-2.10.0.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-common-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/lz4-java-1.4.0.jar:/usr/hdp/current/spark2-client/jars/jackson-jaxrs-base-2.10.0.jar:/usr/hdp/current/spark2-client/jars/jersey-common-2.22.2.jar:/usr/hdp/current/spark2-client/jars/jackson-core-asl-1.9.13.jar:/usr/hdp/current/spark2-client/jars/commons-io-2.4.jar:/usr/hdp/current/spark2-client/jars/hk2-locator-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/RoaringBitmap-0.5.11.jar:/usr/hdp/current/spark2-client/jars/metrics-core-3.1.5.jar:/usr/hdp/current/spark2-client/jars/xz-1.0.jar:/usr/hdp/current/spark2-client/jars/jodd-core-3.5.2.jar:/usr/hdp/current/spark2-client/jars/shapeless_2.11-2.3.2.jar:/usr/hdp/current/spark2-client/jars/aircompressor-0.8.jar:/usr/hdp/current/spark2-client/jars/ST4-4.0.4.jar:/usr/hdp/current/spark2-client/jars/gcs-connector-hadoop3-1.9.17.3.1.5.0-152-shaded.jar:/usr/hdp/current/spark2-client/jars/log4j-1.2.17.jar:/usr/hdp/current/spark2-client/jars/hive-exec-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/accessors-smart-1.2.jar:/usr/hdp/current/spark2-client/jars/metrics-jvm-3.1.5.jar:/usr/hdp/current/spark2-client/jars/commons-math3-3.4.1.jar:/usr/hdp/current/spark2-client/jars/dnsjava-2.1.7.jar:/usr/hdp/current/spark2-client/jars/parquet-encoding-1.8.3.jar:/usr/hdp/current/spark2-client/jars/commons-daemon-1.0.13.jar:/usr/hdp/current/spark2-client/jars/jackson-dataformat-cbor-2.10.0.jar:/usr/hdp/current/spark2-client/jars/javax.ws.rs-api-2.0.1.jar:/usr/hdp/current/spark2-client/jars/commons-compiler-3.0.8.jar:/usr/hdp/current/spark2-client/jars/arrow-format-0.8.0.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/hadoop-cloud-storage-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/kerb-identity-1.0.1.jar:/usr/hdp/current/spark2-client/jars/apache-log4j-extras-1.2.17.jar:/usr/hdp/current/spark2-client/jars/avro-mapred-1.7.7-hadoop2.jar:/usr/hdp/current/spark2-client/jars/kerb-core-1.0.1.jar:/usr/hdp/current/spark2-client/jars/metrics-json-3.1.5.jar:/usr/hdp/current/spark2-client/jars/azure-data-lake-store-sdk-2.3.3.jar:/usr/hdp/current/spark2-client/jars/JavaEWAH-0.3.2.jar:/usr/hdp/current/spark2-client/jars/jta-1.1.jar:/usr/hdp/current/spark2-client/jars/j2objc-annotations-1.3.jar:/usr/hdp/current/spark2-client/jars/spark-graphx_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/transaction-api-1.1.jar:/usr/hdp/current/spark2-client/jars/antlr-2.7.7.jar:/usr/hdp/current/spark2-client/jars/hive-beeline-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/paranamer-2.8.jar:/usr/hdp/current/spark2-client/jars/spark-hive-thriftserver_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/wildfly-openssl-1.0.7.Final.jar:/usr/hdp/current/spark2-client/jars/commons-beanutils-1.9.4.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-registry-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/hadoop-azure-datalake-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hdp/current/spark2-client/jars/jackson-jaxrs-json-provider-2.10.0.jar:/usr/hdp/current/spark2-client/jars/protobuf-java-2.5.0.jar:/usr/hdp/current/spark2-client/jars/commons-lang3-3.5.jar:/usr/hdp/current/spark2-client/jars/commons-pool-1.5.4.jar:/usr/hdp/current/spark2-client/jars/spire-macros_2.11-0.13.0.jar:/usr/hdp/current/spark2-client/jars/janino-3.0.8.jar:/usr/hdp/current/spark2-client/jars/scala-reflect-2.11.12.jar:/usr/hdp/current/spark2-client/jars/jsp-api-2.1.jar:/usr/hdp/current/spark2-client/jars/parquet-jackson-1.8.3.jar:/usr/hdp/current/spark2-client/jars/jackson-module-jaxb-annotations-2.10.0.jar:/usr/hdp/current/spark2-client/jars/breeze_2.11-0.13.2.jar:/usr/hdp/current/spark2-client/jars/HikariCP-java7-2.4.12.jar:/usr/hdp/current/spark2-client/jars/kryo-shaded-3.0.3.jar:/usr/hdp/current/spark2-client/jars/slf4j-log4j12-1.7.16.jar:/usr/hdp/current/spark2-client/jars/hive-cli-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/commons-logging-1.1.3.jar:/usr/hdp/current/spark2-client/jars/json4s-jackson_2.11-3.2.11.jar:/usr/hdp/current/spark2-client/jars/spark-kvstore_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/parquet-column-1.8.3.jar:/usr/hdp/current/spark2-client/jars/pyrolite-4.13.jar:/usr/hdp/current/spark2-client/jars/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hdp/current/spark2-client/jars/macro-compat_2.11-1.1.1.jar:/usr/hdp/current/spark2-client/jars/jackson-module-paranamer-2.10.0.jar:/usr/hdp/current/spark2-client/jars/slf4j-api-1.7.16.jar:/usr/hdp/current/spark2-client/jars/aws-java-sdk-bundle-1.11.375.jar:/usr/hdp/current/spark2-client/jars/hadoop-auth-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/zookeeper-3.4.6.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/avro-1.7.7.jar:/usr/hdp/current/spark2-client/jars/ivy-2.4.0.jar:/usr/hdp/current/spark2-client/jars/guava-28.0-jre.jar:/usr/hdp/current/spark2-client/jars/datanucleus-core-4.1.6.jar:/usr/hdp/current/spark2-client/jars/okhttp-2.7.5.jar:/usr/hdp/current/spark2-client/jars/parquet-common-1.8.3.jar:/usr/hdp/current/spark2-client/jars/httpcore-4.4.8.jar:/usr/hdp/current/spark2-client/jars/spark-catalyst_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/parquet-hadoop-1.8.3.jar:/usr/hdp/current/spark2-client/jars/calcite-avatica-1.2.0-incubating.jar:/usr/hdp/current/spark2-client/jars/parquet-hadoop-bundle-1.6.0.jar:/usr/hdp/current/spark2-client/jars/curator-framework-2.12.0.jar:/usr/hdp/current/spark2-client/jars/guice-4.0.jar:/usr/hdp/current/spark2-client/jars/jdo-api-3.0.1.jar:/usr/hdp/current/spark2-client/jars/antlr-runtime-3.4.jar:/usr/hdp/current/spark2-client/jars/hadoop-mapreduce-client-jobclient-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/hadoop-annotations-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/json4s-core_2.11-3.2.11.jar:/usr/hdp/current/spark2-client/jars/datanucleus-rdbms-4.1.7.jar:/usr/hdp/current/spark2-client/jars/zstd-jni-1.3.2-2.jar:/usr/hdp/current/spark2-client/jars/jul-to-slf4j-1.7.16.jar:/usr/hdp/current/spark2-client/jars/scala-xml_2.11-1.0.5.jar:/usr/hdp/current/spark2-client/jars/animal-sniffer-annotations-1.17.jar:/usr/hdp/current/spark2-client/jars/jakarta.activation-api-1.2.1.jar:/usr/hdp/current/spark2-client/jars/flatbuffers-1.2.0-3f79e055.jar:/usr/hdp/current/spark2-client/jars/httpclient-4.5.4.jar:/usr/hdp/current/spark2-client/jars/kerb-admin-1.0.1.jar:/usr/hdp/current/spark2-client/jars/osgi-resource-locator-1.0.1.jar:/usr/hdp/current/spark2-client/jars/hadoop-client-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/hdp/current/spark2-client/jars/stax2-api-3.1.4.jar:/usr/hdp/current/spark2-client/jars/commons-compress-1.4.1.jar:/usr/hdp/current/spark2-client/jars/hppc-0.7.2.jar:/usr/hdp/current/spark2-client/jars/jtransforms-2.4.0.jar:/usr/hdp/current/spark2-client/jars/jackson-module-scala_2.11-2.10.0.jar:/usr/hdp/current/spark2-client/jars/aopalliance-1.0.jar:/usr/hdp/current/spark2-client/jars/javax.inject-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/bcpkix-jdk15on-1.60.jar:/usr/hdp/current/spark2-client/jars/kerby-asn1-1.0.1.jar:/usr/hdp/current/spark2-client/jars/super-csv-2.2.0.jar:/usr/hdp/current/spark2-client/jars/joda-time-2.9.3.jar:/usr/hdp/current/spark2-client/jars/kerby-pkix-1.0.1.jar:/usr/hdp/current/spark2-client/jars/spark-hive_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/commons-codec-1.10.jar:/usr/hdp/current/spark2-client/jars/machinist_2.11-0.6.1.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-api-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/jackson-mapper-asl-1.9.13.jar:/usr/hdp/current/spark2-client/jars/spark-streaming_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-hadoop-cloud_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/orc-mapreduce-1.4.4-nohive.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-client-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/htrace-core4-4.1.0-incubating.jar:/usr/hdp/current/spark2-client/jars/kerb-common-1.0.1.jar:/usr/hdp/current/spark2-client/jars/error_prone_annotations-2.3.2.jar:/usr/hdp/current/spark2-client/jars/kerb-simplekdc-1.0.1.jar:/usr/hdp/current/spark2-client/jars/breeze-macros_2.11-0.13.2.jar:/usr/hdp/current/spark2-client/jars/json4s-ast_2.11-3.2.11.jar:/usr/hdp/current/spark2-client/jars/snappy-java-1.1.2.6.jar:/usr/hdp/current/spark2-client/jars/datanucleus-api-jdo-4.2.1.jar:/usr/hdp/current/spark2-client/jars/kerb-crypto-1.0.1.jar:/usr/hdp/current/spark2-client/jars/commons-httpclient-3.1.jar:/usr/hdp/current/spark2-client/jars/curator-recipes-2.12.0.jar:/usr/hdp/current/spark2-client/jars/arrow-vector-0.8.0.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/kerb-server-1.0.1.jar:/usr/hdp/current/spark2-client/jars/jersey-server-2.22.2.jar:/usr/hdp/current/spark2-client/jars/jersey-guava-2.22.2.jar:/usr/hdp/current/spark2-client/jars/hadoop-azure-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/orc-core-1.4.4-nohive.jar:/usr/hdp/current/spark2-client/jars/aopalliance-repackaged-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/spire_2.11-0.13.0.jar:/usr/hdp/current/spark2-client/jars/jaxb-api-2.2.11.jar:/usr/hdp/current/spark2-client/jars/scala-compiler-2.11.12.jar:/usr/hdp/current/spark2-client/jars/commons-lang-2.6.jar:/usr/hdp/current/spark2-client/jars/py4j-0.10.7.jar:/usr/hdp/current/spark2-client/jars/compress-lzf-1.0.3.jar:/usr/hdp/current/spark2-client/jars/libthrift-0.12.0.jar:/usr/hdp/current/spark2-client/jars/xbean-asm5-shaded-4.4.jar:/usr/hdp/current/spark2-client/jars/javax.servlet-api-3.1.0.jar:/usr/hdp/current/spark2-client/jars/validation-api-1.1.0.Final.jar:/usr/hdp/current/spark2-client/jars/azure-keyvault-core-1.0.0.jar:/usr/hdp/current/spark2-client/jars/kerb-client-1.0.1.jar:/usr/hdp/current/spark2-client/jars/antlr4-runtime-4.7.jar:/usr/hdp/current/spark2-client/jars/spark-mllib-local_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/metrics-graphite-3.1.5.jar:/usr/hdp/current/spark2-client/jars/netty-3.9.9.Final.jar:/usr/hdp/current/spark2-client/jars/hadoop-openstack-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/bcprov-jdk15on-1.60.jar:/usr/hdp/current/spark2-client/jars/derby-10.12.1.1.jar:/usr/hdp/current/spark2-client/jars/spark-yarn_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/hive-jdbc-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/avro-ipc-1.7.7.jar:/usr/hdp/current/spark2-client/jars/stringtemplate-3.2.1.jar:/usr/hdp/current/spark2-client/jars/libfb303-0.9.3.jar:/usr/hdp/current/spark2-client/jars/HikariCP-2.5.1.jar:/usr/hdp/current/spark2-client/jars/stream-2.7.0.jar:/usr/hdp/current/spark2-client/jars/univocity-parsers-2.5.9.jar:/usr/hdp/current/spark2-client/jars/hk2-utils-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/re2j-1.1.jar:/usr/hdp/current/spark2-client/jars/hk2-api-2.4.0-b34.jar:/usr/hdp/current/spark2-client/jars/jsr305-1.3.9.jar:/usr/hdp/current/spark2-client/jars/calcite-linq4j-1.2.0-incubating.jar:/usr/hdp/current/spark2-client/jars/chill-java-0.8.4.jar:/usr/hdp/current/spark2-client/jars/activation-1.1.1.jar:/usr/hdp/current/spark2-client/jars/hive-metastore-1.21.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-mllib_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/snappy-0.2.jar:/usr/hdp/current/spark2-client/jars/spark-network-common_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/arpack_combined_all-0.1.jar:/usr/hdp/current/spark2-client/jars/leveldbjni-all-1.8.jar:/usr/hdp/current/spark2-client/jars/spark-unsafe_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/jcl-over-slf4j-1.7.16.jar:/usr/hdp/current/spark2-client/jars/jersey-media-jaxb-2.22.2.jar:/usr/hdp/current/spark2-client/jars/kerby-util-1.0.1.jar:/usr/hdp/current/spark2-client/jars/commons-dbcp-1.4.jar:/usr/hdp/current/spark2-client/jars/minlog-1.3.0.jar:/usr/hdp/current/spark2-client/jars/netty-all-4.1.42.Final.jar:/usr/hdp/current/spark2-client/jars/jline-2.14.3.jar:/usr/hdp/current/spark2-client/jars/javolution-5.5.1.jar:/usr/hdp/current/spark2-client/jars/jersey-container-servlet-2.22.2.jar:/usr/hdp/current/spark2-client/jars/hadoop-hdfs-client-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/javax.annotation-api-1.2.jar:/usr/hdp/current/spark2-client/jars/scala-library-2.11.12.jar:/usr/hdp/current/spark2-client/jars/arrow-memory-0.8.0.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/kerby-config-1.0.1.jar:/usr/hdp/current/spark2-client/jars/jetty-util-9.3.24.v20180605.jar:/usr/hdp/current/spark2-client/jars/hadoop-common-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/token-provider-1.0.1.jar:/usr/hdp/current/spark2-client/jars/hadoop-mapreduce-client-common-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/mssql-jdbc-6.2.1.jre7.jar:/usr/hdp/current/spark2-client/jars/jcip-annotations-1.0-1.jar:/usr/hdp/current/spark2-client/jars/jersey-client-2.22.2.jar:/usr/hdp/current/spark2-client/jars/commons-collections-3.2.2.jar:/usr/hdp/current/spark2-client/jars/parquet-format-2.3.1.jar:/usr/hdp/current/spark2-client/jars/commons-crypto-1.0.0.jar:/usr/hdp/current/spark2-client/jars/commons-net-2.2.jar:/usr/hdp/current/spark2-client/jars/jersey-container-servlet-core-2.22.2.jar:/usr/hdp/current/spark2-client/jars/jackson-annotations-2.10.0.jar:/usr/hdp/current/spark2-client/jars/objenesis-2.1.jar:/usr/hdp/current/spark2-client/jars/curator-client-2.12.0.jar:/usr/hdp/current/spark2-client/jars/javax.jdo-3.2.0-m3.jar:/usr/hdp/current/spark2-client/jars/javassist-3.18.1-GA.jar:/usr/hdp/current/spark2-client/jars/opencsv-2.3.jar:/usr/hdp/current/spark2-client/jars/scalap-2.11.12.jar:/usr/hdp/current/spark2-client/jars/kerby-xdr-1.0.1.jar:/usr/hdp/current/spark2-client/jars/bonecp-0.8.0.RELEASE.jar:/usr/hdp/current/spark2-client/jars/kerb-util-1.0.1.jar:/usr/hdp/current/spark2-client/jars/spark-sql_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/jetty-xml-9.3.24.v20180605.jar:/usr/hdp/current/spark2-client/jars/core-1.1.2.jar:/usr/hdp/current/spark2-client/jars/hadoop-aws-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-tags_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-launcher_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/commons-cli-1.2.jar:/usr/hdp/current/spark2-client/jars/jpam-1.1.jar:/usr/hdp/current/spark2-client/jars/spark-network-shuffle_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/spark-sketch_2.11-2.3.2.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/chill_2.11-0.8.4.jar:/usr/hdp/current/spark2-client/jars/oro-2.0.8.jar:/usr/hdp/current/spark2-client/jars/ehcache-3.3.1.jar:/usr/hdp/current/spark2-client/jars/json-smart-2.3.jar:/usr/hdp/current/spark2-client/jars/jetty-webapp-9.3.24.v20180605.jar:/usr/hdp/current/spark2-client/jars/javax.inject-1.jar:/usr/hdp/current/spark2-client/jars/scala-parser-combinators_2.11-1.1.0.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-server-web-proxy-3.1.1.3.1.5.0-152.jar:/usr/hdp/current/spark2-client/jars/nimbus-jose-jwt-4.41.1.jar:/usr/hdp/current/spark2-client/jars/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hdp/current/spark2-client/jars/hadoop-yarn-server-common-3.1.1.3.1.5.0-152.jar:/usr/hdp/3.1.5.0-152/hadoop/conf/
20/03/30 03:05:08 INFO ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
20/03/30 03:05:08 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp
20/03/30 03:05:08 INFO ZooKeeper: Client environment:java.compiler=<NA>
20/03/30 03:05:08 INFO ZooKeeper: Client environment:os.name=Linux
20/03/30 03:05:08 INFO ZooKeeper: Client environment:os.arch=amd64
20/03/30 03:05:08 INFO ZooKeeper: Client environment:os.version=5.0.0-27-lowlatency
20/03/30 03:05:08 INFO ZooKeeper: Client environment:user.name=d917355
20/03/30 03:05:08 INFO ZooKeeper: Client environment:user.home=/home/d917355
20/03/30 03:05:08 INFO ZooKeeper: Client environment:user.dir=/home/d917355/workspace/spark/sampleworkcount
20/03/30 03:05:08 INFO ZooKeeper: Initiating client connection, connectString=lxdsydstl-lxm02-s01-mhm20001.s01.oan:2181,lxdsydstl-lxm03-s01-mhm30001.s01.oan:2181,lxdsydstl-lxm04-s01-mhm40001.s01.oan:2181 sessionTimeout=60000 watcher=shadecurator.org.apache.curator.ConnectionState@7f0b93b4
20/03/30 03:05:08 INFO ClientCnxn: Opening socket connection to server lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181. Will not attempt to authenticate using SASL (unknown error)
20/03/30 03:05:08 INFO ClientCnxn: Socket connection established, initiating session, client: /172.16.6.13:60256, server: lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181
20/03/30 03:05:08 INFO ClientCnxn: Session establishment complete on server lxdsydstl-lxm04-s01-mhm40001.s01.oan/172.16.16.5:2181, sessionid = 0x37090a10009a731, negotiated timeout = 60000
20/03/30 03:05:08 INFO ConnectionStateManager: State change: CONNECTED
20/03/30 03:05:08 INFO CuratorFrameworkImpl: backgroundOperationsLoop exiting
20/03/30 03:05:08 INFO ZooKeeper: Session: 0x37090a10009a731 closed
20/03/30 03:05:08 INFO ClientCnxn: EventThread shut down
20/03/30 03:05:08 INFO HiveConnection: Connected to lxdsydstl-lxe05-s01-ehc20001.s01.oan:10501
20/03/30 03:05:08 INFO HiveServer2CredentialProvider: Added HS2 delegation token for d917355 via jdbc:hive2://lxdsydstl-lxm02-s01-mhm20001.s01.oan:2181,lxdsydstl-lxm03-s01-mhm30001.s01.oan:2181,lxdsydstl-lxm04-s01-mhm40001.s01.oan:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-interactive;principal=hive/_HOST@S01.OAN
20/03/30 03:05:08 INFO HiveStreamingCredentialProvider: Obtaining delegation token (secure metastore) for hive streaming..
20/03/30 03:05:08 INFO HiveConf: Found configuration file file:/etc/spark2/3.1.5.0-152/0/hive-site.xml
20/03/30 03:05:09 WARN HiveConf: HiveConf of name hive.server2.http.endpoint does not exist
20/03/30 03:05:09 INFO HiveStreamingCredentialProvider: Getting Hive delegation token for d917355@S01.OAN against hive/_HOST@S01.OAN at thrift://lxdsydstl-lxm01-s01-mhm10001.s01.oan:9083,thrift://lxdsydstl-lxm02-s01-mhm20001.s01.oan:9083
20/03/30 03:05:09 INFO HiveMetaStoreClient: Trying to connect to metastore with URI thrift://lxdsydstl-lxm01-s01-mhm10001.s01.oan:9083
20/03/30 03:05:09 INFO HiveMetaStoreClient: HMSC::open(): Could not find delegation token. Creating KERBEROS-based thrift connection.
20/03/30 03:05:09 INFO HiveMetaStoreClient: Opened a connection to metastore, current connections: 1
20/03/30 03:05:09 INFO HiveMetaStoreClient: Connected to metastore.
20/03/30 03:05:09 INFO HiveStreamingCredentialProvider: Added delegation token (secure metastore) for hive streaming: Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 0f 64 39 31 37 33 35 35 40 53 30 31 2e 4f 41 4e 04 68 69 76 65 0f 64 39 31 37 33 35 35 40 53 30 31 2e 4f 41 4e 8a 01 71 29 66 68 c9 8a 01 71 4d 72 ec c9 8d 01 49 81 8e 01 82 alias: HIVE_DELEGATION_TOKEN
20/03/30 03:05:09 INFO HiveMetaStoreClient: Closed a connection to metastore, current connections: 0
20/03/30 03:05:09 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://ONE-ANALYTICS/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/30 03:05:09 INFO Client: Source and destination file systems are the same. Not copying hdfs://ONE-ANALYTICS/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-yarn-archive.tar.gz
20/03/30 03:05:09 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://ONE-ANALYTICS/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-hive-archive.tar.gz
20/03/30 03:05:09 INFO Client: Source and destination file systems are the same. Not copying hdfs://ONE-ANALYTICS/hdp/apps/3.1.5.0-152/spark2/spark2-hdp-hive-archive.tar.gz
20/03/30 03:05:09 INFO Client: Uploading resource file:/home/d917355/workspace/spark/sampleworkcount/sparkwordcount-0.0.1-SNAPSHOT.jar -> hdfs://ONE-ANALYTICS/user/d917355/.sparkStaging/application_1585509150056_0009/sparkwordcount-0.0.1-SNAPSHOT.jar
20/03/30 03:05:09 INFO Client: Uploading resource file:/usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly-1.0.0.3.1.5.0-152.jar -> hdfs://ONE-ANALYTICS/user/d917355/.sparkStaging/application_1585509150056_0009/hive-warehouse-connector-assembly-1.0.0.3.1.5.0-152.jar
20/03/30 03:05:13 INFO Client: Uploading resource file:/home/d917355/workspace/spark/sampleworkcount/log4j.properties -> hdfs://ONE-ANALYTICS/user/d917355/.sparkStaging/application_1585509150056_0009/log4j.properties
20/03/30 03:05:13 INFO Client: Uploading resource file:/tmp/spark-3fd5b5f4-0158-4aac-8e54-27b3c0894f07/__spark_conf__2334383048502601611.zip -> hdfs://ONE-ANALYTICS/user/d917355/.sparkStaging/application_1585509150056_0009/__spark_conf__.zip
20/03/30 03:05:13 INFO SecurityManager: Changing view acls to: d917355
20/03/30 03:05:13 INFO SecurityManager: Changing modify acls to: d917355
20/03/30 03:05:13 INFO SecurityManager: Changing view acls groups to: 
20/03/30 03:05:13 INFO SecurityManager: Changing modify acls groups to: 
20/03/30 03:05:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls enabled; users  with view permissions: Set(d917355); groups with view permissions: Set(); users  with modify permissions: Set(d917355); groups with modify permissions: Set()
20/03/30 03:05:13 INFO Client: Submitting application application_1585509150056_0009 to ResourceManager
20/03/30 03:05:14 INFO YarnClientImpl: Submitted application application_1585509150056_0009
20/03/30 03:05:15 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:15 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: adhoc
	 start time: 1585537513956
	 final status: UNDEFINED
	 tracking URL: https://lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090/proxy/application_1585509150056_0009/
	 user: d917355
20/03/30 03:05:16 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:17 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:18 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:19 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:20 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:21 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:22 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:23 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:24 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:24 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 172.16.16.15
	 ApplicationMaster RPC port: 0
	 queue: adhoc
	 start time: 1585537513956
	 final status: UNDEFINED
	 tracking URL: https://lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090/proxy/application_1585509150056_0009/
	 user: d917355
20/03/30 03:05:25 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:26 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:27 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:28 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:29 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:30 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:31 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:32 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:33 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:34 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:35 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:36 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:37 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:38 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:39 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:40 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:41 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:42 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:43 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:44 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:45 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:45 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: AM container is launched, waiting for AM container to Register with RM
	 ApplicationMaster host: N/A
	 ApplicationMaster RPC port: -1
	 queue: adhoc
	 start time: 1585537513956
	 final status: UNDEFINED
	 tracking URL: https://lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090/proxy/application_1585509150056_0009/
	 user: d917355
20/03/30 03:05:46 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:47 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:48 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:49 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:50 INFO Client: Application report for application_1585509150056_0009 (state: ACCEPTED)
20/03/30 03:05:51 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:51 INFO Client: 
	 client token: Token { kind: YARN_CLIENT_TOKEN, service:  }
	 diagnostics: N/A
	 ApplicationMaster host: 172.16.16.9
	 ApplicationMaster RPC port: 0
	 queue: adhoc
	 start time: 1585537513956
	 final status: UNDEFINED
	 tracking URL: https://lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090/proxy/application_1585509150056_0009/
	 user: d917355
20/03/30 03:05:52 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:53 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:54 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:55 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:56 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:57 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:58 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:05:59 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:00 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:01 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:02 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:03 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:04 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:05 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:06 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:07 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:08 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:09 INFO Client: Application report for application_1585509150056_0009 (state: RUNNING)
20/03/30 03:06:10 INFO Client: Application report for application_1585509150056_0009 (state: FINISHED)
20/03/30 03:06:10 INFO Client: 
	 client token: N/A
	 diagnostics: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, lxdsydstl-lxw01-s01-whw10008.s01.oan, executor 1): java.lang.RuntimeException: java.io.IOException: No service instances found in registry
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:67)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: No service instances found in registry
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getServiceInstance(LlapBaseInputFormat.java:436)
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getRecordReader(LlapBaseInputFormat.java:160)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.getRecordReader(HiveWarehouseDataReader.java:77)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.<init>(HiveWarehouseDataReader.java:55)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.getDataReader(HiveWarehouseDataReaderFactory.java:73)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:65)
	... 18 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2039)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2060)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2079)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)
	at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:725)
	at org.apache.spark.sql.Dataset.show(Dataset.scala:702)
	at com.cloudera.sparkwordcount.SimpleHiveApp$.main(SimpleHiveApp.scala:23)
	at com.cloudera.sparkwordcount.SimpleHiveApp.main(SimpleHiveApp.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721)
Caused by: java.lang.RuntimeException: java.io.IOException: No service instances found in registry
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:67)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceRDD.compute(DataSourceRDD.scala:42)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: No service instances found in registry
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getServiceInstance(LlapBaseInputFormat.java:436)
	at org.apache.hadoop.hive.llap.LlapBaseInputFormat.getRecordReader(LlapBaseInputFormat.java:160)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.getRecordReader(HiveWarehouseDataReader.java:77)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReader.<init>(HiveWarehouseDataReader.java:55)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.getDataReader(HiveWarehouseDataReaderFactory.java:73)
	at com.hortonworks.spark.sql.hive.llap.HiveWarehouseDataReaderFactory.createDataReader(HiveWarehouseDataReaderFactory.java:65)
	... 18 more

	 ApplicationMaster host: 172.16.16.9
	 ApplicationMaster RPC port: 0
	 queue: adhoc
	 start time: 1585537513956
	 final status: FAILED
	 tracking URL: https://lxdsydstl-lxm01-s01-mhm10001.s01.oan:8090/proxy/application_1585509150056_0009/
	 user: d917355
Exception in thread "main" org.apache.spark.SparkException: Application application_1585509150056_0009 finished with failed status
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1269)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1627)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:900)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
20/03/30 03:06:10 INFO ShutdownHookManager: Shutdown hook called
20/03/30 03:06:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-0c5ba2d6-f1ae-457d-8041-a7b6d8125485
20/03/30 03:06:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-3fd5b5f4-0158-4aac-8e54-27b3c0894f07
